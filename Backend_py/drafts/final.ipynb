{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x0000022089F8D6D0>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from mysql.connector import connect, Error\n",
    "import collections\n",
    "from typing import Optional, List\n",
    "# Connect to database\n",
    "try:\n",
    "    with connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"mysql\",\n",
    "        database=\"Sprint1BasicEComDb\"\n",
    "    ) as connection:\n",
    "        print(connection)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "    \n",
    "def save_to_csv(data, filename, header):\n",
    "    df = pd.DataFrame(data).set_axis(header, axis=1)\n",
    "    df.to_csv(filename, index=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_list(feature_lists, num_examples_per_list, book_features, random_state):\n",
    "    indices = random_state.choice(\n",
    "        range(len(feature_lists['book_id'])),\n",
    "        size=num_examples_per_list,\n",
    "        replace=False\n",
    "    )\n",
    "    return {feature: [feature_lists[feature][i] for i in indices] for feature in book_features}, [feature_lists[\"user_rating\"][i] for i in indices]\n",
    "\n",
    "\n",
    "def sample_listwise(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    user_features: List[str],\n",
    "    book_features: List[str],\n",
    "    num_list_per_user: int = 50,\n",
    "    num_examples_per_list: int = 5,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Function for converting a dataset to a listwise dataset with user and book features.\n",
    "    Args:\n",
    "        ... (arguments remain the same)\n",
    "    Returns:\n",
    "        A tf.data.Dataset containing list examples with additional user and book features.\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    example_lists_by_user = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(list))\n",
    "\n",
    "    for example in rating_dataset.as_numpy_iterator():\n",
    "        user_id = example[\"user_id\"]\n",
    "        for feature in user_features + book_features + [\"user_rating\"]:\n",
    "            example_lists_by_user[user_id][feature].append(example[feature])\n",
    "\n",
    "    # Initialize tensor_slices with empty lists for each feature\n",
    "    tensor_slices = {feature: []\n",
    "                     for feature in user_features + book_features + [\"user_rating\"]}\n",
    "\n",
    "    for user_id, feature_lists in example_lists_by_user.items():\n",
    "        for _ in range(num_list_per_user):\n",
    "            if len(feature_lists[\"book_id\"]) < num_examples_per_list:\n",
    "                continue\n",
    "\n",
    "            sampled_indices = random_state.choice(\n",
    "                len(feature_lists[\"book_id\"]),\n",
    "                size=num_examples_per_list,\n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            for feature in book_features:\n",
    "                tensor_slices[feature].extend(\n",
    "                    [feature_lists[feature][i] for i in sampled_indices])\n",
    "\n",
    "            tensor_slices[\"user_rating\"].extend(\n",
    "                [feature_lists[\"user_rating\"][i] for i in sampled_indices])\n",
    "\n",
    "            for feature in user_features:\n",
    "                tensor_slices[feature].extend(\n",
    "                    [feature_lists[feature][0]] * num_examples_per_list)\n",
    "\n",
    "    # Convert lists to numpy arrays or Tensors\n",
    "    tensor_slices = {k: tf.convert_to_tensor(\n",
    "        np.array(v)) for k, v in tensor_slices.items()}\n",
    "\n",
    "    # Finally, return a tf.data.Dataset object\n",
    "    return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "\n",
    "def sample_listwise_2(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    user_features: List[str],\n",
    "    book_features: List[str],\n",
    "    num_list_per_user: int = 50,\n",
    "    num_examples_per_list: int = 5,\n",
    "    seed: Optional[int] = None\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Convert a dataset to a listwise dataset with user and book features.\"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    example_lists_by_user = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(list))\n",
    "\n",
    "    # Collect features for each user\n",
    "    for example in rating_dataset.as_numpy_iterator():\n",
    "        user_id = example[\"user_id\"]\n",
    "        for feature in user_features + book_features + [\"user_rating\"]:\n",
    "            example_lists_by_user[user_id][feature].append(example[feature])\n",
    "\n",
    "    structured_lists = []\n",
    "    # Build structured lists for each user\n",
    "    for user_id, feature_lists in example_lists_by_user.items():\n",
    "        for _ in range(num_list_per_user):\n",
    "            if len(feature_lists[\"book_id\"]) < num_examples_per_list:\n",
    "                continue  # Skip users with fewer books than required for a full list\n",
    "\n",
    "            sampled_indices = random_state.choice(len(feature_lists[\"book_id\"]),\n",
    "                                                  size=num_examples_per_list,\n",
    "                                                  replace=False)\n",
    "\n",
    "            list_entry = {feature: []\n",
    "                          for feature in user_features + book_features + [\"user_rating\"]}\n",
    "            for feature in book_features + [\"user_rating\"]:\n",
    "                for i in sampled_indices:\n",
    "                    list_entry[feature].append(feature_lists[feature][i])\n",
    "\n",
    "            # Add user features (assumed to be the same for all books in the list)\n",
    "            for feature in user_features:\n",
    "                list_entry[feature] = feature_lists[feature][0]\n",
    "\n",
    "            structured_lists.append(list_entry)\n",
    "\n",
    "    # Convert structured lists to a format suitable for tf.data.Dataset\n",
    "    \n",
    "\n",
    "    def generator():\n",
    "        for entry in structured_lists:\n",
    "            # Convert lists for listwise features to the required fixed length\n",
    "            # Ensure single-value features are correctly formatted\n",
    "            # This assumes `entry` is already structured to match these expectations\n",
    "            yield entry\n",
    "    \n",
    "    output_types = {\n",
    "        'book_id': tf.int64,\n",
    "        'book_title': tf.string,\n",
    "        'user_rating': tf.float32,\n",
    "        'user_id': tf.int64,\n",
    "        'sex': tf.int64  # Assuming 'sex' is an example of a single-value user feature\n",
    "    }\n",
    "\n",
    "    output_shapes = {\n",
    "        'book_id': (num_examples_per_list,),\n",
    "        'book_title': (num_examples_per_list,),\n",
    "        'user_rating': (num_examples_per_list,),\n",
    "        'user_id': (),\n",
    "        'sex': ()  # Ensure shapes are specified for all features\n",
    "    }\n",
    "\n",
    "    # Create the dataset with known types and shapes\n",
    "    return tf.data.Dataset.from_generator(generator, output_types=output_types, output_shapes=output_shapes)\n",
    "\n",
    "# Assuming the rating_dataset is preloaded and structured correctly.\n",
    "\n",
    "\n",
    "# Here's how you would call the function with the specific features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_all_books_user_and_ratings(): #TODO: like get All Books, add attributes to the query\n",
    "    query = \"\"\"SELECT \n",
    "    ubr.userid, \n",
    "    u.birthdate, \n",
    "    u.sex, \n",
    "    u.genrePreference, \n",
    "    u.authorPreference,\n",
    "    ubr.bookid,\n",
    "    b.book,\n",
    "    b.description,\n",
    "    b.num_page,\n",
    "    b.pre_rating,\n",
    "    b.publication,\n",
    "    b.authorid,\n",
    "    b.genreid,\n",
    "    ubr.rating AS user_rating\n",
    "FROM userbookratings ubr\n",
    "JOIN (\n",
    "    -- User Details and Preferences Subquery\n",
    "    SELECT \n",
    "        u.id AS userid, \n",
    "        u.birthdate, \n",
    "        u.sex, \n",
    "        bp.genrePreference, \n",
    "        bp.authorPreference\n",
    "    FROM users u\n",
    "    JOIN bookpreferences bp ON u.id = bp.userID\n",
    ") u ON ubr.userid = u.userid\n",
    "JOIN (\n",
    "    -- Book Details Subquery\n",
    "    SELECT \n",
    "        bk.id AS bookid, \n",
    "        bk.book,\n",
    "        bk.description,\n",
    "        bk.numPages AS num_page,\n",
    "        bk.rating AS pre_rating,\n",
    "        bk.publication,\n",
    "        GROUP_CONCAT(DISTINCT ba.authorId ORDER BY ba.authorId ASC) AS authorid,\n",
    "        GROUP_CONCAT(DISTINCT bg.genreId ORDER BY bg.genreId ASC) AS genreid\n",
    "    FROM bookitems bk\n",
    "    LEFT JOIN bookauthors ba ON ba.bookId = bk.id\n",
    "    LEFT JOIN bookgenres bg ON bg.bookId = bk.id\n",
    "    GROUP BY bk.id\n",
    ") b ON ubr.bookid = b.bookid; \"\"\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    user_ratings = mydb.fetchall()\n",
    "    return user_ratings\n",
    "\n",
    "\n",
    "print()\n",
    "save_to_csv(get_all_books_user_and_ratings(), '../User_book_Ratings.csv', ['user_id','birth_date','sex','genre_preference','author_preference','book_id','book_title','description','num_pages','pre_rating','publication','author_id','genre_id','user_rating'])\n",
    "\n",
    "rating_rank = pd.read_csv('../User_book_Ratings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot unbatch an input with scalar components.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 53\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#################################################################\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Tensor Shape doesn't look right book != TensorSpec(shape=(5,) #\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#################################################################\u001b[39;00m\n\u001b[0;32m     50\u001b[0m train \u001b[38;5;241m=\u001b[39m sample_listwise_2(train, user_features, book_features,\n\u001b[0;32m     51\u001b[0m                               num_list_per_user\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_examples_per_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m cached_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(train)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2957\u001b[0m, in \u001b[0;36mDatasetV2.unbatch\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (\u001b[39;00m\n\u001b[0;32m   2954\u001b[0m \u001b[38;5;66;03m# dataset_ops -> unbatch_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unbatch_op\n\u001b[1;32m-> 2957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munbatch_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\unbatch_op.py:26\u001b[0m, in \u001b[0;36m_unbatch\u001b[1;34m(input_dataset, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.unbatch()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m normalized_dataset \u001b[38;5;241m=\u001b[39m dataset_ops\u001b[38;5;241m.\u001b[39mnormalize_to_dense(input_dataset)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_UnbatchDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\unbatch_op.py:36\u001b[0m, in \u001b[0;36m_UnbatchDataset.__init__\u001b[1;34m(self, input_dataset, name)\u001b[0m\n\u001b[0;32m     34\u001b[0m flat_shapes \u001b[38;5;241m=\u001b[39m input_dataset\u001b[38;5;241m.\u001b[39m_flat_shapes  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(s\u001b[38;5;241m.\u001b[39mndims \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m flat_shapes):\n\u001b[1;32m---> 36\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot unbatch an input with scalar components.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m known_batch_dim \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mDimension(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m flat_shapes:\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot unbatch an input with scalar components."
     ]
    }
   ],
   "source": [
    "# all the ones I want to use\n",
    "user_features = ['userid', 'birthdate', 'sex',\n",
    "                 'genrePreference', 'authorPreference']\n",
    "book_features = ['book_id', 'book_title', 'description',\n",
    "                 'num_page', 'pre_rating', 'publication', 'authorids', 'genreids']\n",
    "# the ones I'm ac\n",
    "user_features = ['user_id', 'sex']\n",
    "book_features = ['book_id', 'book_title']\n",
    "\n",
    "\n",
    "# convert dataframe to tf.data.Dataset\n",
    "rating_rank = tf.data.Dataset.from_tensor_slices(dict(rating_rank))# LINE CAN ONLY BE USED 1NCE\n",
    "\n",
    "# TODO: Add more features to the dataset\n",
    "rating_rank = rating_rank.map(lambda x: {\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"sex\": x['sex'],\n",
    "    \"book_id\": x['book_id'],\n",
    "    \"book_title\": x['book_title'],\n",
    "    \"user_rating\": x[\"user_rating\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "book_ids = rating_rank.batch(1_000_000).map(lambda x: x[\"book_id\"])\n",
    "unique_book_ids = np.unique(np.concatenate(list(book_ids)))\n",
    "\n",
    "book_titles = rating_rank.batch(1_000_000).map(lambda x: x[\"book_title\"])\n",
    "unique_book_titles = np.unique(np.concatenate(list(book_titles)))\n",
    "\n",
    "#num_pages = rating_rank.batch(1_000_000).map(lambda x: x[\"num_pages\"])\n",
    "#unique_num_pages = np.unique(np.concatenate(list(num_pages)))\n",
    "\n",
    "\n",
    "user_ids = rating_rank.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "#######################################\n",
    "# NUMBER TAKINGS NEEDS TO BE ADJUSTED #\n",
    "#######################################\n",
    "shuffled = rating_rank.shuffle(100, seed=42, reshuffle_each_iteration=False)\n",
    "train = shuffled.take(80)\n",
    "test = shuffled.skip(80).take(20)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Tensor Shape doesn't look right book != TensorSpec(shape=(5,) #\n",
    "#################################################################\n",
    "train = sample_listwise_2(train, user_features, book_features,\n",
    "                              num_list_per_user=2, num_examples_per_list=5, seed=42)\n",
    "\n",
    "train = train.unbatch()\n",
    "\n",
    "cached_train = train.shuffle(100).batch(32).cache()\n",
    "\n",
    "print(train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for now the book features we are doing are book_id, book_title, num_pages \n",
    "\n",
    "\n",
    "\n",
    "#book_id\n",
    "book_id_lookup = tf.keras.layers.IntegerLookup()\n",
    "book_id_lookup.adapt(unique_book_ids) # list of all book ids, the strings\n",
    "\n",
    "# Book title \n",
    "book_title_lookup = tf.keras.layers.StringLookup()\n",
    "book_title_lookup.adapt(unique_book_titles)# should be unique book titles, the strings\n",
    "\n",
    "\n",
    "#num pages # not sure honestly\n",
    "#num_pages_lookup = tf.keras.layers.Normalization(axis=None)\n",
    "#num_pages_lookup.adapt(unique_num_pages.astype(np.float32)) # list of all num pages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #book_id\n",
    "        # self.book_id_embedding = tf.keras.Sequential([\n",
    "        #     book_id_lookup,\n",
    "        #     tf.keras.layers.Embedding(book_id_lookup.vocabulary_size(), 32)\n",
    "        # ])\n",
    "        \n",
    "        self.book_id_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.IntegerLookup(vocabulary=unique_book_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_book_ids) + 1, 32)\n",
    "        ])\n",
    "        \n",
    "        self.book_title_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=unique_book_titles, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_book_titles) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        # self.book_title_embedding = tf.keras.Sequential([\n",
    "        #     book_title_lookup,\n",
    "        #     tf.keras.layers.Embedding(book_title_lookup.vocabulary_size(), 32)\n",
    "        # ])\n",
    "        \n",
    "        #self.num_pages_embedding = tf.keras.Sequential([\n",
    "        #    num_pages_lookup,\n",
    "        #    tf.keras.layers.Embedding(num_pages_lookup.vocabulary_size(), 32)\n",
    "        #])\n",
    "      \n",
    "     \n",
    "    def call(self, inputs):\n",
    "        print(\"Model input keys:\", list(inputs.keys()))\n",
    "        for key, value in inputs.items():\n",
    "            print(f\"Shape of {key}:\", value.shape)\n",
    "        return tf.concat([\n",
    "            self.book_id_embedding(inputs[\"book_id\"]),\n",
    "            self.book_title_embedding(inputs[\"book_title\"]),\n",
    "            # self.num_pages_embedding(inputs[\"num_pages\"]),\n",
    "        ], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For now features are user_id, sex;\n",
    "# User ID \n",
    "user_id_lookup = tf.keras.layers.IntegerLookup()\n",
    "user_id_lookup.adapt(unique_user_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    # don't these need to be passed in?\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # User embedding based on user_id\n",
    "        self.user_id_embedding = tf.keras.Sequential([\n",
    "            user_id_lookup,\n",
    "            tf.keras.layers.Embedding(user_id_lookup.vocabulary_size(), 32),\n",
    "        ])\n",
    "\n",
    "        # Sex as a boolean feature, could be treated directly as an input or embedded\n",
    "        self.sex_embedding = tf.keras.layers.Embedding(\n",
    "            2, 32)  # Assuming sex is represented as 0 or 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"Model input keys:\", list(inputs.keys()))\n",
    "        for key, value in inputs.items():\n",
    "            print(f\"Shape of {key}:\", value.shape)\n",
    "            \n",
    "        user_id_feature = self.user_id_embedding(inputs[\"user_id\"])\n",
    "        sex_feature = self.sex_embedding(tf.cast(inputs[\"sex\"], tf.int32))  # Corrected attribute name\n",
    "\n",
    "        return tf.concat([\n",
    "            user_id_feature,\n",
    "            sex_feature,\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # User embeddings\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "            UserModel()\n",
    "            ])\n",
    "        # Restaurand embeddings\n",
    "        self.book_embeddings = tf.keras.Sequential([\n",
    "            BookModel(),\n",
    "            \n",
    "        ])\n",
    "        # Compute predictions\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "            # Learn multiple dense layers.\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            # Make rating predictions in the final layer.\n",
    "            tf.keras.layers.Dense(1)\n",
    "\n",
    "        ])\n",
    "\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "                tf.keras.metrics.RootMeanSquaredError()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        # Extract user embeddings [batch_size, embedding_dim].\n",
    "        user_embeddings = self.user_embeddings({\n",
    "            'user_id': features['user_id'],\n",
    "            'sex': features['sex'],\n",
    "        })\n",
    "\n",
    "        # Extract book embeddings [batch_size, num_books, embedding_dim].\n",
    "        book_embeddings = self.book_embeddings({\n",
    "            'book_id': features['book_id'],\n",
    "            'book_title': features['book_title'],\n",
    "            # Potentially include other book features here\n",
    "        })\n",
    "\n",
    "        # Determine the number of books in the list for each user.\n",
    "        list_length = features['book_id'].shape[1]\n",
    "\n",
    "        # Repeat the user embeddings to match the shape of book embeddings.\n",
    "        # New shape: [batch_size, num_books, embedding_dim].\n",
    "        user_embedding_repeated = tf.repeat(\n",
    "            tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "        print(\"User Embedding Repeated Shape:\", user_embedding_repeated.shape)\n",
    "        print(\"Book Embeddings Shape:\", book_embeddings.shape)\n",
    "\n",
    "        # Concatenate user and book embeddings along the last dimension.\n",
    "        combined_embeddings = tf.concat(\n",
    "            [user_embedding_repeated, book_embeddings], axis=2)\n",
    "\n",
    "        combined_embeddings_flat = tf.reshape(combined_embeddings, [tf.shape(combined_embeddings)[0] * list_length, -1])\n",
    "\n",
    "        # Passing the flattened embeddings to the scoring model.\n",
    "        scores_flat = self.score_model(combined_embeddings_flat)\n",
    "\n",
    "        # Reshaping scores to match the labels shape (10, 5)\n",
    "        scores = tf.reshape(scores_flat, [tf.shape(features['book_id'])[0], list_length])\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        labels = features.pop(\"user_rating\")\n",
    "        scores = self(features)\n",
    "        return self.task(\n",
    "            labels=labels,\n",
    "            predictions=tf.squeeze(scores, axis=-1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input keys: ['user_id', 'sex']\n",
      "Shape of user_id: (None,)\n",
      "Shape of sex: (None,)\n",
      "Model input keys: ['book_id', 'book_title']\n",
      "Shape of book_id: (None, 5)\n",
      "Shape of book_title: (None, 5)\n",
      "User Embedding Repeated Shape: (None, 5, 64)\n",
      "Book Embeddings Shape: (None, 5, 64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python311\\Lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_23064\\2683820854.py\", line 76, in compute_loss\n        scores = self(features)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\__autograph_generated_fileeljtbnzm.py\", line 18, in tf__call\n        scores_flat = ag__.converted_call(ag__.ld(self).score_model, (ag__.ld(combined_embeddings_flat),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'ranking_model' (type RankingModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_23064\\2683820854.py\", line 66, in call  *\n            scores_flat = self.score_model(combined_embeddings_flat)\n        File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 148, in build\n            raise ValueError(\n    \n        ValueError: Exception encountered when calling layer 'sequential_5' (type Sequential).\n        \n        The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)\n        \n        Call arguments received by layer 'sequential_5' (type Sequential):\n          • inputs=tf.Tensor(shape=(None, None), dtype=float32)\n          • training=None\n          • mask=None\n    \n    \n    Call arguments received by layer 'ranking_model' (type RankingModel):\n      • features={'book_id': 'tf.Tensor(shape=(None, 5), dtype=int64)', 'book_title': 'tf.Tensor(shape=(None, 5), dtype=string)', 'user_id': 'tf.Tensor(shape=(None,), dtype=int64)', 'sex': 'tf.Tensor(shape=(None,), dtype=int64)'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m listwise_model \u001b[38;5;241m=\u001b[39m RankingModel(tfr\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mListMLELoss())\n\u001b[0;32m      2\u001b[0m listwise_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad(\u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mlistwise_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filem3e8rhbb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_recommenders\\models\\base.py:68\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom train step using the `compute_loss` method.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 68\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m   \u001b[38;5;66;03m# Handle regularization losses as well.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m   regularization_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "Cell \u001b[1;32mIn[9], line 76\u001b[0m, in \u001b[0;36mRankingModel.compute_loss\u001b[1;34m(self, features, training)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, features, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     75\u001b[0m     labels \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_rating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask(\n\u001b[0;32m     78\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m     79\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39msqueeze(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileeljtbnzm.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     16\u001b[0m combined_embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, ([ag__\u001b[38;5;241m.\u001b[39mld(user_embedding_repeated), ag__\u001b[38;5;241m.\u001b[39mld(book_embeddings)],), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), fscope)\n\u001b[0;32m     17\u001b[0m combined_embeddings_flat \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (ag__\u001b[38;5;241m.\u001b[39mld(combined_embeddings), [ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(combined_embeddings),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(list_length), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 18\u001b[0m scores_flat \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_embeddings_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m scores \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (ag__\u001b[38;5;241m.\u001b[39mld(scores_flat), [ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_id\u001b[39m\u001b[38;5;124m'\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(list_length)]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python311\\Lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_23064\\2683820854.py\", line 76, in compute_loss\n        scores = self(features)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\__autograph_generated_fileeljtbnzm.py\", line 18, in tf__call\n        scores_flat = ag__.converted_call(ag__.ld(self).score_model, (ag__.ld(combined_embeddings_flat),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'ranking_model' (type RankingModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_23064\\2683820854.py\", line 66, in call  *\n            scores_flat = self.score_model(combined_embeddings_flat)\n        File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 148, in build\n            raise ValueError(\n    \n        ValueError: Exception encountered when calling layer 'sequential_5' (type Sequential).\n        \n        The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)\n        \n        Call arguments received by layer 'sequential_5' (type Sequential):\n          • inputs=tf.Tensor(shape=(None, None), dtype=float32)\n          • training=None\n          • mask=None\n    \n    \n    Call arguments received by layer 'ranking_model' (type RankingModel):\n      • features={'book_id': 'tf.Tensor(shape=(None, 5), dtype=int64)', 'book_title': 'tf.Tensor(shape=(None, 5), dtype=string)', 'user_id': 'tf.Tensor(shape=(None,), dtype=int64)', 'sex': 'tf.Tensor(shape=(None,), dtype=int64)'}\n"
     ]
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "listwise_model.fit(cached_train, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input keys: ['user_id', 'sex']\n",
      "Shape of user_id: (20,)\n",
      "Shape of sex: (20,)\n",
      "Model input keys: ['book_id', 'book_title']\n",
      "Shape of book_id: (20, 5)\n",
      "Shape of book_title: (20, 5)\n",
      "User Embedding Repeated Shape: (20, 5, 64)\n",
      "Book Embeddings Shape: (20, 5, 64)\n",
      "book_id: (20, 5)\n",
      "book_title: (20, 5)\n",
      "user_rating: (20, 5)\n",
      "user_id: (20,)\n",
      "sex: (20,)\n",
      "Predictions shape: (20, 5)\n",
      "Ratings shape: (20, 5)\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tensorflow_ranking\\python\\losses_impl.py:1550: The name tf.where is deprecated. Please use tf.compat.v1.where instead.\n",
      "\n",
      "tf.Tensor(4.783707, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for feature in cached_train.take( 1):\n",
    "    # print(feature)\n",
    "    predictions = listwise_model(feature, training=True)\n",
    "    for key, value in feature.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    print(\"Predictions shape:\", predictions.shape) \n",
    "    print(\"Ratings shape:\", feature[\"user_rating\"].shape) \n",
    "    loss = tfr.keras.losses.ListMLELoss()(feature[\"user_rating\"], predictions)\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
