{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from mysql.connector import connect, Error\n",
    "import collections\n",
    "from typing import Optional, List\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Connect to database\n",
    "try:\n",
    "    with connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"mysql\",\n",
    "        database=\"Sprint1BasicEComDb\"\n",
    "    ) as connection:\n",
    "        print(connection)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "    \n",
    "def save_to_csv(data, filename, header):\n",
    "    df = pd.DataFrame(data).set_axis(header, axis=1)\n",
    "    df.to_csv(filename, index=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_list(feature_lists, num_examples_per_list, book_features, random_state):\n",
    "    indices = random_state.choice(\n",
    "        range(len(feature_lists['book_id'])),\n",
    "        size=num_examples_per_list,\n",
    "        replace=False\n",
    "    )\n",
    "    return {feature: [feature_lists[feature][i] for i in indices] for feature in book_features}, [feature_lists[\"user_rating\"][i] for i in indices]\n",
    "\n",
    "\n",
    "def sample_listwise(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    user_features: List[str],\n",
    "    book_features: List[str],\n",
    "    num_list_per_user: int = 50,\n",
    "    num_examples_per_list: int = 5,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Function for converting a dataset to a listwise dataset with user and book features.\n",
    "    Args:\n",
    "        ... (arguments remain the same)\n",
    "    Returns:\n",
    "        A tf.data.Dataset containing list examples with additional user and book features.\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    example_lists_by_user = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(list))\n",
    "\n",
    "    for example in rating_dataset.as_numpy_iterator():\n",
    "        user_id = example[\"user_id\"]\n",
    "        for feature in user_features + book_features + [\"user_rating\"]:\n",
    "            example_lists_by_user[user_id][feature].append(example[feature])\n",
    "\n",
    "    # Initialize tensor_slices with empty lists for each feature\n",
    "    tensor_slices = {feature: []\n",
    "                     for feature in user_features + book_features + [\"user_rating\"]}\n",
    "\n",
    "    for user_id, feature_lists in example_lists_by_user.items():\n",
    "        for _ in range(num_list_per_user):\n",
    "            if len(feature_lists[\"book_id\"]) < num_examples_per_list:\n",
    "                continue\n",
    "\n",
    "            sampled_indices = random_state.choice(\n",
    "                len(feature_lists[\"book_id\"]),\n",
    "                size=num_examples_per_list,\n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            for feature in book_features:\n",
    "                tensor_slices[feature].extend(\n",
    "                    [feature_lists[feature][i] for i in sampled_indices])\n",
    "\n",
    "            tensor_slices[\"user_rating\"].extend(\n",
    "                [feature_lists[\"user_rating\"][i] for i in sampled_indices])\n",
    "\n",
    "            for feature in user_features:\n",
    "                tensor_slices[feature].extend(\n",
    "                    [feature_lists[feature][0]] * num_examples_per_list)\n",
    "\n",
    "    # Convert lists to numpy arrays or Tensors\n",
    "    tensor_slices = {k: tf.convert_to_tensor(\n",
    "        np.array(v)) for k, v in tensor_slices.items()}\n",
    "\n",
    "    # Finally, return a tf.data.Dataset object\n",
    "    return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "\n",
    "def sample_listwise_2(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    user_features: List[str],\n",
    "    book_features: List[str],\n",
    "    num_list_per_user: int = 50,\n",
    "    num_examples_per_list: int = 5,\n",
    "    seed: Optional[int] = None\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Convert a dataset to a listwise dataset with user and book features.\"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    example_lists_by_user = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(list))\n",
    "\n",
    "    # Collect features for each user\n",
    "    for example in rating_dataset.as_numpy_iterator():\n",
    "        user_id = example[\"user_id\"]\n",
    "        for feature in user_features + book_features + [\"user_rating\"]:\n",
    "            example_lists_by_user[user_id][feature].append(example[feature])\n",
    "\n",
    "    structured_lists = []\n",
    "    # Build structured lists for each user\n",
    "    for user_id, feature_lists in example_lists_by_user.items():\n",
    "        for _ in range(num_list_per_user):\n",
    "            if len(feature_lists[\"book_id\"]) < num_examples_per_list:\n",
    "                continue  # Skip users with fewer books than required for a full list\n",
    "\n",
    "            sampled_indices = random_state.choice(len(feature_lists[\"book_id\"]),\n",
    "                                                  size=num_examples_per_list,\n",
    "                                                  replace=False)\n",
    "\n",
    "            list_entry = {feature: []\n",
    "                          for feature in user_features + book_features + [\"user_rating\"]}\n",
    "            for feature in book_features + [\"user_rating\"]:\n",
    "                for i in sampled_indices:\n",
    "                    list_entry[feature].append(feature_lists[feature][i])\n",
    "\n",
    "            # Add user features (assumed to be the same for all books in the list)\n",
    "            for feature in user_features:\n",
    "                list_entry[feature] = feature_lists[feature][0]\n",
    "\n",
    "            structured_lists.append(list_entry)\n",
    "\n",
    "    # Convert structured lists to a format suitable for tf.data.Dataset\n",
    "    \n",
    "\n",
    "    def generator():\n",
    "        for entry in structured_lists:\n",
    "            # Convert lists for listwise features to the required fixed length\n",
    "            # Ensure single-value features are correctly formatted\n",
    "            # This assumes `entry` is already structured to match these expectations\n",
    "            yield entry\n",
    "    \n",
    "    output_types = {\n",
    "        'book_id': tf.int64,\n",
    "        'book_title': tf.string,\n",
    "        'user_rating': tf.float32,\n",
    "        'user_id': tf.int64,\n",
    "        'sex': tf.int64  # Assuming 'sex' is an example of a single-value user feature\n",
    "    }\n",
    "\n",
    "    output_shapes = {\n",
    "        'book_id': (num_examples_per_list,),\n",
    "        'book_title': (num_examples_per_list,),\n",
    "        'user_rating': (num_examples_per_list,),\n",
    "        'user_id': (),\n",
    "        'sex': ()  # Ensure shapes are specified for all features\n",
    "    }\n",
    "\n",
    "    # Create the dataset with known types and shapes\n",
    "    return tf.data.Dataset.from_generator(generator, output_types=output_types, output_shapes=output_shapes)\n",
    "\n",
    "# Assuming the rating_dataset is preloaded and structured correctly.\n",
    "\n",
    "\n",
    "# Here's how you would call the function with the specific features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_books_user_and_ratings(): #TODO: like get All Books, add attributes to the query\n",
    "    query = \"\"\"SELECT \n",
    "    ubr.userid, \n",
    "    u.birthdate, \n",
    "    u.sex, \n",
    "    u.genrePreference, \n",
    "    u.authorPreference,\n",
    "    ubr.bookid,\n",
    "    b.book,\n",
    "    b.description,\n",
    "    b.num_page,\n",
    "    b.pre_rating,\n",
    "    b.publication,\n",
    "    b.authorid,\n",
    "    b.genreid,\n",
    "    ubr.rating AS user_rating\n",
    "FROM userbookratings ubr\n",
    "JOIN (\n",
    "    -- User Details and Preferences Subquery\n",
    "    SELECT \n",
    "        u.id AS userid, \n",
    "        u.birthdate, \n",
    "        u.sex, \n",
    "        bp.genrePreference, \n",
    "        bp.authorPreference\n",
    "    FROM users u\n",
    "    JOIN bookpreferences bp ON u.id = bp.userID\n",
    ") u ON ubr.userid = u.userid\n",
    "JOIN (\n",
    "    -- Book Details Subquery\n",
    "    SELECT \n",
    "        bk.id AS bookid, \n",
    "        bk.book,\n",
    "        bk.description,\n",
    "        bk.numPages AS num_page,\n",
    "        bk.rating AS pre_rating,\n",
    "        bk.publication,\n",
    "        GROUP_CONCAT(DISTINCT ba.authorId ORDER BY ba.authorId ASC) AS authorid,\n",
    "        GROUP_CONCAT(DISTINCT bg.genreId ORDER BY bg.genreId ASC) AS genreid\n",
    "    FROM bookitems bk\n",
    "    LEFT JOIN bookauthors ba ON ba.bookId = bk.id\n",
    "    LEFT JOIN bookgenres bg ON bg.bookId = bk.id\n",
    "    GROUP BY bk.id\n",
    ") b ON ubr.bookid = b.bookid; \"\"\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    user_ratings = mydb.fetchall()\n",
    "    return user_ratings\n",
    "\n",
    "\n",
    "print()\n",
    "save_to_csv(get_all_books_user_and_ratings(), '../User_book_Ratings.csv', ['user_id','birth_date','sex','genre_preference','author_preference','book_id','book_title','description','num_pages','pre_rating','publication','author_id','genre_id','user_rating'])\n",
    "\n",
    "rating_rank = pd.read_csv('../User_book_Ratings.csv')\n",
    "rating_rank = rating_rank.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the ones I want to use\n",
    "user_features = ['userid', 'birthdate', 'sex',\n",
    "                 'genrePreference', 'authorPreference']\n",
    "book_features = ['book_id', 'book_title', 'description',\n",
    "                 'num_page', 'pre_rating', 'publication', 'authorids', 'genreids']\n",
    "# the ones I'm ac\n",
    "user_features = ['user_id', 'sex']\n",
    "book_features = ['book_id', 'book_title']\n",
    "\n",
    "\n",
    "\n",
    "# convert dataframe to tf.data.Dataset\n",
    "rating_rank = tf.data.Dataset.from_tensor_slices(dict(rating_rank))# LINE CAN ONLY BE USED 1NCE\n",
    "\n",
    "# TODO: Add more features to the dataset\n",
    "rating_rank = rating_rank.map(lambda x: {\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"sex\": x['sex'],\n",
    "    \"book_id\": x['book_id'],\n",
    "    \"book_title\": x['book_title'],\n",
    "    \"user_rating\": x[\"user_rating\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "book_ids = rating_rank.batch(1_000_000).map(lambda x: x[\"book_id\"])\n",
    "unique_book_ids = np.unique(np.concatenate(list(book_ids)))\n",
    "\n",
    "book_titles = rating_rank.batch(1_000_000).map(lambda x: x[\"book_title\"])\n",
    "unique_book_titles = np.unique(np.concatenate(list(book_titles)))\n",
    "\n",
    "#num_pages = rating_rank.batch(1_000_000).map(lambda x: x[\"num_pages\"])\n",
    "#unique_num_pages = np.unique(np.concatenate(list(num_pages)))\n",
    "\n",
    "\n",
    "user_ids = rating_rank.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "#######################################\n",
    "# NUMBER TAKINGS NEEDS TO BE ADJUSTED #\n",
    "#######################################\n",
    "print(len(rating_rank))\n",
    "shuffled = rating_rank.shuffle(40000, seed=42, reshuffle_each_iteration=False)\n",
    "train = shuffled.take(30000)\n",
    "test = shuffled.skip(30000).take(10000)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Tensor Shape doesn't look right book != TensorSpec(shape=(5,) #\n",
    "#################################################################\n",
    "train = sample_listwise_2(train, user_features, book_features,\n",
    "                              num_list_per_user=50, num_examples_per_list=5, seed=42)\n",
    "\n",
    "test = sample_listwise_2(test, user_features, book_features,\n",
    "                             num_list_per_user=1, num_examples_per_list=5, seed=42)\n",
    "\n",
    "cached_train = train.shuffle(100).batch(32, drop_remainder=True).cache()\n",
    "\n",
    "cached_test = test.batch(32).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now the book features we are doing are book_id, book_title, num_pages \n",
    "\n",
    "\n",
    "\n",
    "#book_id\n",
    "book_id_lookup = tf.keras.layers.IntegerLookup()\n",
    "book_id_lookup.adapt(unique_book_ids) # list of all book ids, the strings\n",
    "\n",
    "# Book title \n",
    "book_title_lookup = tf.keras.layers.StringLookup()\n",
    "book_title_lookup.adapt(unique_book_titles)# should be unique book titles, the strings\n",
    "\n",
    "\n",
    "#num pages # not sure honestly\n",
    "#num_pages_lookup = tf.keras.layers.Normalization(axis=None)\n",
    "#num_pages_lookup.adapt(unique_num_pages.astype(np.float32)) # list of all num pages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # book_id\n",
    "        # self.book_id_embedding = tf.keras.Sequential([\n",
    "        #     book_id_lookup,\n",
    "        #     tf.keras.layers.Embedding(book_id_lookup.vocabulary_size(), 32)\n",
    "        # ])\n",
    "\n",
    "        self.book_id_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.IntegerLookup(\n",
    "                vocabulary=unique_book_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_book_ids) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.book_title_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_book_titles, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_book_titles) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        # self.book_title_embedding = tf.keras.Sequential([\n",
    "        #     book_title_lookup,\n",
    "        #     tf.keras.layers.Embedding(book_title_lookup.vocabulary_size(), 32)\n",
    "        # ])\n",
    "\n",
    "        # self.num_pages_embedding = tf.keras.Sequential([\n",
    "        #    num_pages_lookup,\n",
    "        #    tf.keras.layers.Embedding(num_pages_lookup.vocabulary_size(), 32)\n",
    "        # ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"Model input keys:\", list(inputs.keys()))\n",
    "        for key, value in inputs.items():\n",
    "            print(f\"Shape of {key}:\", value.shape)\n",
    "        return tf.concat([\n",
    "            self.book_id_embedding(inputs[\"book_id\"]),\n",
    "            self.book_title_embedding(inputs[\"book_title\"]),\n",
    "            # self.num_pages_embedding(inputs[\"num_pages\"]),\n",
    "        ], axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(BookModel, self).get_config()\n",
    "        config.update({\n",
    "            \"unique_book_ids\": self.book_id_embedding.layers[0].get_vocabulary(),\n",
    "            \"unique_book_titles\": self.book_title_embedding.layers[0].get_vocabulary()\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For now features are user_id, sex;\n",
    "# User ID \n",
    "user_id_lookup = tf.keras.layers.IntegerLookup()\n",
    "user_id_lookup.adapt(unique_user_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    # don't these need to be passed in?\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # User embedding based on user_id\n",
    "        self.user_id_embedding = tf.keras.Sequential([\n",
    "            user_id_lookup,\n",
    "            tf.keras.layers.Embedding(user_id_lookup.vocabulary_size(), 32),\n",
    "        ])\n",
    "\n",
    "        # Sex as a boolean feature, could be treated directly as an input or embedded\n",
    "        self.sex_embedding = tf.keras.layers.Embedding(\n",
    "            2, 32)  # Assuming sex is represented as 0 or 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"Model input keys:\", list(inputs.keys()))\n",
    "        for key, value in inputs.items():\n",
    "            print(f\"Shape of {key}:\", value.shape)\n",
    "\n",
    "        user_id_feature = self.user_id_embedding(inputs[\"user_id\"])\n",
    "        sex_feature = self.sex_embedding(\n",
    "            tf.cast(inputs[\"sex\"], tf.int32))  # Corrected attribute name\n",
    "\n",
    "        return tf.concat([\n",
    "            user_id_feature,\n",
    "            sex_feature,\n",
    "        ], axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(UserModel, self).get_config()\n",
    "        config.update({\n",
    "            \"user_ids_vocabulary\": self.user_id_embedding.layers[0].get_vocabulary(),\n",
    "            \"sex_embedding_dim\": self.sex_embedding.output_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # User embeddings\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "            UserModel()\n",
    "            ])\n",
    "        # Restaurand embeddings\n",
    "        self.book_embeddings = tf.keras.Sequential([\n",
    "            BookModel(),\n",
    "            \n",
    "        ])\n",
    "        # Compute predictions\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "            # Learn multiple dense layers.\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1, activation=\"sigmoid\"),  # Sigmoid activation to ensure output is between 0 and 1\n",
    "            tf.keras.layers.Lambda(lambda x: x * 5)  # Scale up by 5 to adjust the range to 0-5\n",
    "\n",
    "        ])\n",
    "\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "                tf.keras.metrics.RootMeanSquaredError()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        # Extract user embeddings [batch_size, embedding_dim].\n",
    "        user_embeddings = self.user_embeddings({\n",
    "            'user_id': features['user_id'],\n",
    "            'sex': features['sex'],\n",
    "        })\n",
    "\n",
    "        # Extract book embeddings [batch_size, num_books, embedding_dim].\n",
    "        book_embeddings = self.book_embeddings({\n",
    "            'book_id': features['book_id'],\n",
    "            'book_title': features['book_title'],\n",
    "            # Potentially include other book features here\n",
    "        })\n",
    "\n",
    "        # Determine the number of books in the list for each user.\n",
    "        list_length = features['book_id'].shape[1]\n",
    "\n",
    "        # Repeat the user embeddings to match the shape of book embeddings.\n",
    "        # New shape: [batch_size, num_books, embedding_dim].\n",
    "        user_embedding_repeated = tf.repeat(\n",
    "            tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "        print(\"User Embedding Repeated Shape:\", user_embedding_repeated.shape)\n",
    "        print(\"Book Embeddings Shape:\", book_embeddings.shape)\n",
    "\n",
    "        # Concatenate user and book embeddings along the last dimension.\n",
    "        combined_embeddings = tf.concat(\n",
    "            [user_embedding_repeated, book_embeddings], axis=2)\n",
    "\n",
    "        combined_embeddings_flat = tf.reshape(combined_embeddings, [tf.shape(combined_embeddings)[0] * list_length, -1])\n",
    "\n",
    "        # Passing the flattened embeddings to the scoring model.\n",
    "        scores_flat = self.score_model(combined_embeddings_flat)\n",
    "\n",
    "        # Reshaping scores to match the labels shape (10, 5)\n",
    "        scores = tf.reshape(scores_flat, [tf.shape(features['book_id'])[0], list_length])\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        labels = features.pop(\"user_rating\")\n",
    "        scores = self(features)\n",
    "        return self.task(\n",
    "            labels=labels,\n",
    "            predictions=scores,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x223d53af710>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "listwise_model.fit(cached_train, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input keys: ['user_id', 'sex']\n",
      "Shape of user_id: (None,)\n",
      "Shape of sex: (None,)\n",
      "Model input keys: ['book_id', 'book_title']\n",
      "Shape of book_id: (None, 5)\n",
      "Shape of book_title: (None, 5)\n",
      "User Embedding Repeated Shape: (None, 5, 64)\n",
      "Book Embeddings Shape: (None, 5, 64)\n",
      "4/4 [==============================] - 1s 34ms/step - ndcg_metric: 0.8775 - root_mean_squared_error: 2.7867 - loss: 4.7406 - regularization_loss: 0.0000e+00 - total_loss: 4.7406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ndcg_metric': 0.8774760365486145,\n",
       " 'root_mean_squared_error': 2.7866885662078857,\n",
       " 'loss': 4.914475440979004,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 4.914475440979004}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "listwise_model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input keys: ['user_id', 'sex']\n",
      "Shape of user_id: (None,)\n",
      "Shape of sex: (None,)\n",
      "Model input keys: ['book_id', 'book_title']\n",
      "Shape of book_id: (None, 5)\n",
      "Shape of book_title: (None, 5)\n",
      "User Embedding Repeated Shape: (None, 5, 64)\n",
      "Book Embeddings Shape: (None, 5, 64)\n",
      "Model input keys: ['user_id', 'sex']\n",
      "Shape of user_id: (None,)\n",
      "Shape of sex: (None,)\n",
      "Model input keys: ['user_id', 'sex']\n",
      "Shape of user_id: (None,)\n",
      "Shape of sex: (None,)\n",
      "Model input keys: ['book_id', 'book_title']\n",
      "Shape of book_id: (None, 5)\n",
      "Shape of book_title: (None, 5)\n",
      "Model input keys: ['book_id', 'book_title']\n",
      "Shape of book_id: (None, 5)\n",
      "Shape of book_title: (None, 5)\n",
      "User Embedding Repeated Shape: (None, 5, 64)\n",
      "Book Embeddings Shape: (None, 5, 64)\n",
      "User Embedding Repeated Shape: (None, 5, 64)\n",
      "Book Embeddings Shape: (None, 5, 64)\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: ../../RESTful API/listwise_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../RESTful API/listwise_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "listwise_model.save(\"../../RESTful API/listwise_model\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * {'book_id': <tf.Tensor 'features_2:0' shape=(5,) dtype=int64>,\n 'book_title': <tf.Tensor 'features_3:0' shape=(5,) dtype=string>,\n 'sex': <tf.Tensor 'features_1:0' shape=(1,) dtype=int64>,\n 'user_id': <tf.Tensor 'features:0' shape=(1,) dtype=int64>}\n  Keyword arguments: {'training': False}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * {'book_id': TensorSpec(shape=(None, 5), dtype=tf.int64, name='book_id'),\n 'book_title': TensorSpec(shape=(None, 5), dtype=tf.string, name='book_title'),\n 'sex': TensorSpec(shape=(None,), dtype=tf.int64, name='sex'),\n 'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name='user_id')}\n  Keyword arguments: {'training': True}\n\nOption 2:\n  Positional arguments (1 total):\n    * {'book_id': TensorSpec(shape=(None, 5), dtype=tf.int64, name='book_id'),\n 'book_title': TensorSpec(shape=(None, 5), dtype=tf.string, name='book_title'),\n 'sex': TensorSpec(shape=(None,), dtype=tf.int64, name='sex'),\n 'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name='user_id')}\n  Keyword arguments: {'training': False}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m'\u001b[39m, custom_objects\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUserModel\u001b[39m\u001b[38;5;124m'\u001b[39m: UserModel,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBookModel\u001b[39m\u001b[38;5;124m'\u001b[39m: BookModel,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRankingModel\u001b[39m\u001b[38;5;124m'\u001b[39m: RankingModel,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCGMetric\u001b[39m\u001b[38;5;124m'\u001b[39m: tfr\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mNDCGMetric\n\u001b[0;32m      6\u001b[0m })\n\u001b[1;32m----> 9\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure dtype is tf.int64\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Ensure dtype is tf.int64 # Ensure dtype is tf.int64\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbook_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m102\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m103\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m104\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m105\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbook_title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBook A\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBook B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBook C\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBook D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBook E\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py:335\u001b[0m, in \u001b[0;36mrecreate_function.<locals>.restored_function_body\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m   positional, keyword \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mstructured_input_signature\n\u001b[0;32m    332\u001b[0m   signature_descriptions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    333\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOption \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword arguments: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    334\u001b[0m           index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, _pretty_format_positional(positional), keyword))\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find matching concrete function to call loaded from the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel. Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_pretty_format_positional(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Expected these arguments to match one of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(saved_function\u001b[38;5;241m.\u001b[39mconcrete_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m option(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mjoin(signature_descriptions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * {'book_id': <tf.Tensor 'features_2:0' shape=(5,) dtype=int64>,\n 'book_title': <tf.Tensor 'features_3:0' shape=(5,) dtype=string>,\n 'sex': <tf.Tensor 'features_1:0' shape=(1,) dtype=int64>,\n 'user_id': <tf.Tensor 'features:0' shape=(1,) dtype=int64>}\n  Keyword arguments: {'training': False}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * {'book_id': TensorSpec(shape=(None, 5), dtype=tf.int64, name='book_id'),\n 'book_title': TensorSpec(shape=(None, 5), dtype=tf.string, name='book_title'),\n 'sex': TensorSpec(shape=(None,), dtype=tf.int64, name='sex'),\n 'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name='user_id')}\n  Keyword arguments: {'training': True}\n\nOption 2:\n  Positional arguments (1 total):\n    * {'book_id': TensorSpec(shape=(None, 5), dtype=tf.int64, name='book_id'),\n 'book_title': TensorSpec(shape=(None, 5), dtype=tf.string, name='book_title'),\n 'sex': TensorSpec(shape=(None,), dtype=tf.int64, name='sex'),\n 'user_id': TensorSpec(shape=(None,), dtype=tf.int64, name='user_id')}\n  Keyword arguments: {'training': False}"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_model = load_model('export', custom_objects={\n",
    "    'UserModel': UserModel,\n",
    "    'BookModel': BookModel,\n",
    "    'RankingModel': RankingModel,\n",
    "    'NDCGMetric': tfr.keras.metrics.NDCGMetric\n",
    "})\n",
    "\n",
    "\n",
    "loaded_model({\n",
    "    \"user_id\": tf.constant([1], dtype=tf.int64),  # Ensure dtype is tf.int64\n",
    "    \"sex\": tf.constant([0], dtype=tf.int64),      # Ensure dtype is tf.int64 # Ensure dtype is tf.int64\n",
    "    \"book_id\": tf.constant([[101, 102, 103, 104, 105]], dtype=tf.int64),\n",
    "    \"book_title\": tf.constant([[\"Book A\", \"Book B\", \"Book C\", \"Book D\", \"Book E\"]])\n",
    "}, training=False),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
