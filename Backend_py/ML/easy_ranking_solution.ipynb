{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x0000022C51678B50>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import keras\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from mysql.connector import connect, Error\n",
    "\n",
    "\n",
    "# Connect to database\n",
    "try:\n",
    "    with connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"mysql\",\n",
    "        database=\"Sprint1BasicEComDb\"\n",
    "    ) as connection:\n",
    "        print(connection)\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ratings(): #TODO: like get All Books, add attributes to the query\n",
    "    query = \"SELECT userId,bookId,rating FROM UserBookRatings\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    user_ratings = mydb.fetchall()\n",
    "    return user_ratings\n",
    "\n",
    "\n",
    "def get_all_books_metadata():\n",
    "    query =\"\"\" \n",
    "    \"\"\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    all_books = mydb.fetchall()\n",
    "    return all_books\n",
    "\n",
    "\n",
    "def get_all_users_metadata():\n",
    "    query = \"SELECT id,birthDate,sex FROM Users\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    all_users = mydb.fetchall()\n",
    "    return all_users\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename, header):\n",
    "    user_ratings_ids = pd.DataFrame(data).set_axis(header, axis=1)\n",
    "    user_ratings_ids.to_csv(filename, index=False, )\n",
    "    \n",
    "def newUserIDs(maxID,quantity):\n",
    "    newID = []\n",
    "    for i in range(quantity):\n",
    "        newID.append(maxID+i+1)\n",
    "    return newID\n",
    "    \n",
    "#save_to_csv(get_user_ratings(), 'user_ratings.csv', ['user_id', 'movie_title', 'rating'])\n",
    "#save_to_csv(get_all_books(), 'all_books.csv', ['book_id', 'book_title', 'description', 'num_pages', 'rating', 'num_of_voters','genres','formats','authors'])\n",
    "#save_to_csv(get_all_users(), 'all_users.csv', ['user_id', 'birth_date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_user_ratings()\n",
    "\n",
    "user_ratings_ids = pd.DataFrame(df, columns=[\"user_id\", \"movie_title\", \"user_rating\"])\n",
    "\n",
    "# users_to_remove = user_ratings_ids['user_id'].drop_duplicates().sample(30)\n",
    "# books_to_remove = user_ratings_ids['movie_title'].drop_duplicates().sample(100)\n",
    "# df_new_users_known_books = user_ratings_ids[~user_ratings_ids['user_id'].isin(users_to_remove) & ~user_ratings_ids['movie_title'].isin(books_to_remove)]\n",
    "# df_old_users_new_books = user_ratings_ids[user_ratings_ids['user_id'].isin(users_to_remove) & ~user_ratings_ids['movie_title'].isin(books_to_remove)]\n",
    "\n",
    "# convert to string\n",
    "user_ratings_ids['user_id'] = user_ratings_ids['user_id'].astype(str)\n",
    "user_ratings_ids['movie_title'] = user_ratings_ids['movie_title'].astype(str)\n",
    "\n",
    "# df_new_users_known_books['user_id'] = df_new_users_known_books['user_id'].astype(str)\n",
    "# df_new_users_known_books['movie_title'] = df_new_users_known_books['movie_title'].astype(str)\n",
    "\n",
    "# df_old_users_new_books['user_id'] = df_old_users_new_books['user_id'].astype(str)\n",
    "# df_old_users_new_books['movie_title'] = df_old_users_new_books['movie_title'].astype(str)\n",
    "\n",
    "########\n",
    "\n",
    "rating_rank = user_ratings_ids[['user_id', 'movie_title', 'user_rating']].copy()\n",
    "book_rank = user_ratings_ids[['movie_title']].copy()\n",
    "\n",
    "# rating_rank10 = df_new_users_known_books[['user_id', 'movie_title', 'user_rating']].copy()\n",
    "# book_rank10 = df_new_users_known_books[['movie_title']].copy()\n",
    "\n",
    "# rating_rank01 = df_old_users_new_books[['user_id', 'movie_title', 'user_rating']].copy()\n",
    "# book_rank01 = df_old_users_new_books[['movie_title']].copy()\n",
    "\n",
    "# user_ratings_ids=[]\n",
    "# df_new_users_known_books=[]\n",
    "# df_old_users_new_books=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset from the dataframe\n",
    "# Will cause error if ran again\n",
    "\n",
    "\n",
    "books = tf.data.Dataset.from_tensor_slices(dict(book_rank)) # book_rank only \n",
    "books = books.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "# books10 = tf.data.Dataset.from_tensor_slices(dict(book_rank10)) # book_rank only # problem\n",
    "# books10 = books10.map(lambda x: x[\"movie_title\"]) # \n",
    "\n",
    "\n",
    "# books01 = tf.data.Dataset.from_tensor_slices(dict(book_rank01)) # book_rank only \n",
    "# books01 = books01.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "rating_rank = tf.data.Dataset.from_tensor_slices(dict(rating_rank))\n",
    "rating_rank = rating_rank.map(lambda x: {\n",
    "    'user_id': x['user_id'],\n",
    "    'movie_title': x['movie_title'],\n",
    "    'user_rating': x['user_rating'],\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# rating_rank10 = tf.data.Dataset.from_tensor_slices(dict(rating_rank10))\n",
    "# rating_rank10 = rating_rank10.map(lambda x: {\n",
    "#     'user_id': x['user_id'],\n",
    "#     'movie_title': x['movie_title'],\n",
    "#     'user_rating': x['user_rating'],\n",
    "# })\n",
    "\n",
    "# rating_rank01 = tf.data.Dataset.from_tensor_slices(dict(rating_rank01))\n",
    "# rating_rank01 = rating_rank01.map(lambda x: {\n",
    "#     'user_id': x['user_id'],\n",
    "#     'movie_title': x['movie_title'],\n",
    "#     'user_rating': x['user_rating'],\n",
    "# })\n",
    "\n",
    "## new \n",
    "\n",
    "unique_books = np.unique(np.concatenate(list(books.batch(1000)))) # is it correct IDK # should be strings\n",
    "#unique_books10 = np.unique(np.concatenate(list(books10.batch(1000)))) # is it correct IDK # should be strings\n",
    "#unique_books01 = np.unique(np.concatenate(list(books01.batch(1000)))) # is it correct IDK # should be strings\n",
    "\n",
    "unique_user_ids = np.unique(np.concatenate(list(rating_rank.batch(1_000).map(lambda x: x['user_id'])))) ## could be book id\n",
    "#unique_user_ids10 = np.unique(np.concatenate(list(rating_rank10.batch(1_000).map(lambda x: x['user_id']))))\n",
    "#unique_user_ids01 = np.unique(np.concatenate(list(rating_rank01.batch(1_000).map(lambda x: x['user_id']))))\n",
    "\n",
    "# decode from bytes to string\n",
    "unique_books = [book.decode('utf-8') for book in unique_books]\n",
    "unique_user_ids = [user_id.decode('utf-8') for user_id in unique_user_ids]\n",
    "\n",
    "#unique_books10 = [book.decode('utf-8') for book in unique_books10]\n",
    "#unique_user_ids10 = [user_id.decode('utf-8') for user_id in unique_user_ids10]\n",
    "\n",
    "#unique_books01 = [book.decode('utf-8') for book in unique_books01]\n",
    "#unique_user_ids01 = [user_id.decode('utf-8') for user_id in unique_user_ids01]\n",
    "\n",
    "\n",
    "#print(unique_books10)\n",
    "#print(unique_user_ids10)\n",
    "#\n",
    "#\n",
    "#print(unique_books01)\n",
    "#print(unique_user_ids01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "800\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "# do we have 100_000 ratings? # user_ratings needs to be a new type\n",
    "shuffled = rating_rank.shuffle(\n",
    "    500, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "#shuffled10 = rating_rank10.shuffle(\n",
    "#    50, seed=42, reshuffle_each_iteration=False)\n",
    "#\n",
    "#shuffled01 = rating_rank01.shuffle(\n",
    "#    50, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "\n",
    "\n",
    "train = shuffled.take(300)\n",
    "test = shuffled.skip(300).take(200)\n",
    "\n",
    "#train10 = shuffled10.take(100)\n",
    "#test10 = shuffled10.skip(3).take(2)\n",
    "#print(len(train10))\n",
    "#\n",
    "#train01 = shuffled01.take(10)\n",
    "#test01 = shuffled01.skip(3).take(2)\n",
    "\n",
    "print(len(train))\n",
    "\n",
    "# We sample 50 lists for each user for the training data. For each list we\n",
    "# sample 5 movies from the movies the user rated.\n",
    "\n",
    "\n",
    "#train10 = tfrs.examples.movielens.sample_listwise(\n",
    "#    train10,\n",
    "#    num_list_per_user=100,\n",
    "#    num_examples_per_list=5,\n",
    "#    seed=42\n",
    "#)\n",
    "#\n",
    "#test10 = tfrs.examples.movielens.sample_listwise( ## making test empty\n",
    "#    test10,\n",
    "#    num_list_per_user=1,\n",
    "#    num_examples_per_list=5,\n",
    "#    seed=42\n",
    "#)\n",
    "#\n",
    "#train01 = tfrs.examples.movielens.sample_listwise(\n",
    "#    train01,\n",
    "#    num_list_per_user=100,\n",
    "#    num_examples_per_list=5,\n",
    "#    seed=42\n",
    "#)\n",
    "\n",
    "#test01 = tfrs.examples.movielens.sample_listwise( ## making test empty\n",
    "#    test01,\n",
    "#    num_list_per_user=1,\n",
    "#    num_examples_per_list=5,\n",
    "#    seed=42\n",
    "#)\n",
    "\n",
    "train = tfrs.examples.movielens.sample_listwise(\n",
    "    train,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=5, # writing a 4<= gives 0 results\n",
    "    seed=42\n",
    ")\n",
    "test = tfrs.examples.movielens.sample_listwise( ## making test empty\n",
    "    test,\n",
    "    num_list_per_user=5,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_books),\n",
    "      tf.keras.layers.Embedding(len(unique_books) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "    # tensor.\n",
    "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
    "\n",
    "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the user\n",
    "    # embeddings to match the shape of movie embeddings.\n",
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, movie_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"user_rating\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(1000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "\n",
    "#cached_train10 = train10.shuffle(50).batch(8192).cache()\n",
    "#cached_test10 = test10.batch(4096).cache()\n",
    "#\n",
    "#print(len(cached_train10))\n",
    "#\n",
    "#cached_train01 = train01.shuffle(100).batch(8192).cache()\n",
    "#cached_test01 = test01.batch(4096).cache()\n",
    "#\n",
    "#print(len(cached_train01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22c558d5190>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "listwise_model.fit(cached_train, epochs=5, verbose=False)  \n",
    "#listwise_model.fit(cached_train10, epochs=5, verbose=True)  \n",
    "#listwise_model.fit(cached_train01, epochs=5, verbose=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 666ms/step - ndcg_metric: 0.6844 - root_mean_squared_error: 1.5923 - loss: 4.6837 - regularization_loss: 0.0000e+00 - total_loss: 4.6837\n",
      "NDCG of the ListMLE model: 0.6844\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../RESTful API/easy_listwise_model_saved\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../RESTful API/easy_listwise_model_saved\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "listwise_model_result = listwise_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"NDCG of the ListMLE model: {:.4f}\".format(listwise_model_result[\"ndcg_metric\"]))\n",
    "#################\n",
    "# Graph Results #\n",
    "#################\n",
    "\n",
    "custom_objects = {\"ListMLELoss\": tfr.keras.losses.ListMLELoss}\n",
    "listwise_model.save(\"../RESTful API/easy_listwise_model_saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.1664295402788225, 1.6558558964574057, 1.571770056924314, 4.878856651670684, 2.233772952977673, 0.0, 5.0, 3.35783625063212, 2.400132362784623, 2.8365992670204045, 2.8313081017903805, 2.5272539046492044]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "predictions = listwise_model({\n",
    "    \"user_id\": tf.constant([\"42\"]),\n",
    "    \"movie_title\": tf.constant([[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"]])\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# loaded_model = keras.models.load_model('../RESTful API/easy_listwise_model_saved',custom_objects=custom_objects)\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\"../RESTful API/easy_listwise_model_saved\", custom_objects={\n",
    "    \"ListMLELoss\": tfr.keras.losses.ListMLELoss,\n",
    "    \"NDCGMetric\": tfr.keras.metrics.NDCGMetric,\n",
    "    \"DynamicRepeatLayer\": DynamicRepeatLayer})\n",
    "\n",
    "\n",
    "def prepare_batches(user_id, movie_titles, batch_size=5):\n",
    "    # If the list of movie_titles is shorter than 5, extend it by repeating titles\n",
    "    if len(movie_titles) < batch_size:\n",
    "        movie_titles = (movie_titles * batch_size)[:batch_size]\n",
    "    elif len(movie_titles) % batch_size != 0:\n",
    "        # If the list is not a multiple of 5, extend it by repeating the first few titles\n",
    "        extra_titles_needed = batch_size - (len(movie_titles) % batch_size)\n",
    "        movie_titles.extend(movie_titles[:extra_titles_needed])\n",
    "\n",
    "    # Create batches of 5 movie_titles\n",
    "    title_batches = [movie_titles[i:i + batch_size]\n",
    "                     for i in range(0, len(movie_titles), batch_size)]\n",
    "    # Associate each batch with the same user_id\n",
    "    user_batches = [[user_id] * batch_size for _ in range(len(title_batches))]\n",
    "\n",
    "    return title_batches\n",
    "\n",
    "\n",
    "length_of_books = len([\"1\", \"2\", \"3\", \"4\",\n",
    "                       \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"])\n",
    "batched_books = prepare_batches(\"42\", [\"1\", \"2\", \"3\", \"4\",\n",
    "                                       \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"])\n",
    "\n",
    "\n",
    "# model, user_id:str[], books_lists:str[[]], has to be equal lengths\n",
    "def make_predictions(model, user_id, books_lists):\n",
    "    predictions = []\n",
    "    for i in range(len(books_lists)):\n",
    "        predictions.append(model({\n",
    "            \"user_id\": tf.constant([user_id]),\n",
    "            \"movie_title\": tf.constant([books_lists[i]])\n",
    "        }))\n",
    "    return predictions\n",
    "\n",
    "\n",
    "predictions = make_predictions(loaded_model, \"42\", batched_books)\n",
    "\n",
    "\n",
    "concatenated_tensor = tf.concat(predictions, axis=1)\n",
    "list_of_values = concatenated_tensor.numpy().flatten().tolist()\n",
    "\n",
    "\n",
    "def scale_to_range(input_list, new_min=0, new_max=5):\n",
    "    # Find the current range\n",
    "    old_min, old_max = min(input_list), max(input_list)\n",
    "\n",
    "    # Avoid division by zero in case all values are the same\n",
    "    if old_min == old_max:\n",
    "        # or return any constant list within [new_min, new_max]\n",
    "        return [new_min] * len(input_list)\n",
    "\n",
    "    # Scale values to the new range [0, 5]\n",
    "    scaled_list = [((value - old_min) / (old_max - old_min)) *\n",
    "                   (new_max - new_min) + new_min for value in input_list]\n",
    "\n",
    "    return scaled_list\n",
    "\n",
    "\n",
    "list_of_ratings = scale_to_range(list_of_values)\n",
    "print(list_of_ratings[0:length_of_books])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
