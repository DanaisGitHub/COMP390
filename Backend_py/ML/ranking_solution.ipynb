{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a20c3f",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "1. Hold ListWise Ranking Algorthim\n",
    "2. Train Algorthim\n",
    "3. Release trained Algorthim as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7115d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from mysql.connector import connect, Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b41dec",
   "metadata": {},
   "source": [
    "# Phase 1: Get (train and test) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222980b",
   "metadata": {},
   "source": [
    "### Retrive User, Book, and Rating data\n",
    "\n",
    "##### Current Problem\n",
    "need to get UserAttribute and BookAttribute Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9279b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x0000019A4537E010>\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "try:\n",
    "    with connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"mysql\",\n",
    "        database=\"Sprint1BasicEComDb\"\n",
    "    ) as connection:\n",
    "        print(connection)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "# Get user ratings from database # really should be making this API calls # lets save as CSV files\n",
    "\n",
    "\n",
    "def get_user_ratings(): #TODO: like get All Books, add attributes to the query\n",
    "    query = \"SELECT userId,bookId,rating FROM UserBookRatings\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    user_ratings = mydb.fetchall()\n",
    "    return user_ratings\n",
    "\n",
    "\n",
    "def get_all_books_metadata():\n",
    "    query =\"\"\" \n",
    "    \"\"\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    all_books = mydb.fetchall()\n",
    "    return all_books\n",
    "\n",
    "\n",
    "def get_all_users_metadata():\n",
    "    query = \"SELECT id,birthDate,sex FROM Users\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    all_users = mydb.fetchall()\n",
    "    return all_users\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename, header):\n",
    "    df = pd.DataFrame(data).set_axis(header, axis=1)\n",
    "    df.to_csv(filename, index=False, )\n",
    "\n",
    "\n",
    "#save_to_csv(get_user_ratings(), 'user_ratings.csv', ['user_id', 'movie_title', 'rating'])\n",
    "#save_to_csv(get_all_books(), 'all_books.csv', ['book_id', 'book_title', 'description', 'num_pages', 'rating', 'num_of_voters','genres','formats','authors'])\n",
    "#save_to_csv(get_all_users(), 'all_users.csv', ['user_id', 'birth_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfdfd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id movie_title  user_rating\n",
      "0           1           1         0.09\n",
      "1           2           1         0.17\n",
      "2           3           1         1.29\n",
      "3           4           1         0.21\n",
      "4           5           1         0.19\n",
      "...       ...         ...          ...\n",
      "49995      96         500         0.10\n",
      "49996      97         500         0.02\n",
      "49997      98         500         0.07\n",
      "49998      99         500         0.06\n",
      "49999     100         500         0.03\n",
      "\n",
      "[50000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Turn User ratings into a dataframe from list of tuples\n",
    "\n",
    "df = get_user_ratings()\n",
    "user_ratings_ids = pd.DataFrame(df, columns=[\"user_id\", \"movie_title\", \"user_rating\"])\n",
    "\n",
    "# convert to string\n",
    "user_ratings_ids['user_id'] = user_ratings_ids['user_id'].astype(str)\n",
    "user_ratings_ids['movie_title'] = user_ratings_ids['movie_title'].astype(str)\n",
    "\n",
    "rating_rank = user_ratings_ids[['user_id', 'movie_title', 'user_rating']].copy()\n",
    "book_rank = user_ratings_ids[['movie_title']].copy()\n",
    "\n",
    "user_ratings_ids=[]\n",
    "\n",
    "print(rating_rank)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da29e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3382576429.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    'book_title': x['book_title']\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Create a tf.data.Dataset from the dataframe\n",
    "# Will cause error if ran again\n",
    "\n",
    "books = tf.data.Dataset.from_tensor_slices(dict(book_rank)) # book_rank only \n",
    "books = books.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "\n",
    "rating_rank = tf.data.Dataset.from_tensor_slices(dict(rating_rank))\n",
    "rating_rank = rating_rank.map(lambda x: {\n",
    "    'user_id': x['user_id'],\n",
    "    'movie_title': x['movie_title'],\n",
    "    'user_rating': x['user_rating'],\n",
    "    'book_id': x['book_id']\n",
    "    'book_title': x['book_title'] \n",
    "    'num_pages': x['num_pages'],\n",
    "    'pre_rating': x['pre_rating'],\n",
    "})\n",
    "\n",
    "## new \n",
    "\n",
    "unique_books = np.unique(np.concatenate(list(books.batch(1000)))) # is it correct IDK # should be strings\n",
    "\n",
    "unique_user_ids = np.unique(np.concatenate(list(rating_rank.batch(1_000).map(lambda x: x['user_id'])))) ## could be book id\n",
    "\n",
    "# decode from bytes to string\n",
    "unique_books = [book.decode('utf-8') for book in unique_books]\n",
    "unique_user_ids = [user_id.decode('utf-8') for user_id in unique_user_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ed7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec={'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'movie_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float64, name=None)}>\n",
      "<_ShuffleDataset element_spec={'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'movie_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float64, name=None)}>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "# do we have 100_000 ratings? # user_ratings needs to be a new type\n",
    "shuffled = rating_rank.shuffle(\n",
    "    100_000, seed=42, reshuffle_each_iteration=False)\n",
    "pprint.pprint(rating_rank)\n",
    "pprint.pprint(shuffled)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# We sample 50 lists for each user for the training data. For each list we\n",
    "# sample 5 movies from the movies the user rated.\n",
    "train = tfrs.examples.movielens.sample_listwise(\n",
    "    train,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "test = tfrs.examples.movielens.sample_listwise(\n",
    "    test,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "def sample_listwise(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    num_list_per_user: int = 10,\n",
    "    num_examples_per_list: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Function for converting the MovieLens 100K dataset to a listwise dataset.\n",
    "\n",
    "    Args:\n",
    "        rating_dataset:\n",
    "          The MovieLens ratings dataset loaded from TFDS with features \"movie_title\", \"user_id\", and \"user_rating\".\n",
    "        num_list_per_user:\n",
    "          An integer representing the number of lists that should be sampled for each user in the training dataset.\n",
    "        num_examples_per_list:\n",
    "          An integer representing the number of movies to be sampled for each list from the list of movies rated by the user.\n",
    "        seed:\n",
    "          An integer for creating `np.random.RandomState`.\n",
    "\n",
    "    Returns:\n",
    "        A tf.data.Dataset containing list examples.\n",
    "\n",
    "        Each example contains three keys: \"user_id\", \"movie_title\", and \"user_rating\". \"user_id\" maps to a string tensor that represents the user \n",
    "        id for the example. \"movie_title\" maps to a tensor of shape [sum(num_example_per_list)] with dtype tf.string. \n",
    "        It represents the list of candidate movie ids. \"user_rating\" maps to a tensor of shape [sum(num_example_per_list)] \n",
    "        with dtype tf.float32. It represents the rating of each movie in the candidate list.\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "\n",
    "    example_lists_by_user = collections.defaultdict(_create_feature_dict)\n",
    "\n",
    "    movie_title_vocab = set()\n",
    "    for example in rating_dataset:\n",
    "        user_id = example[\"user_id\"].numpy()\n",
    "        example_lists_by_user[user_id][\"movie_title\"].append(\n",
    "            example[\"movie_title\"])\n",
    "        example_lists_by_user[user_id][\"user_rating\"].append(\n",
    "            example[\"user_rating\"])\n",
    "        movie_title_vocab.add(example[\"movie_title\"].numpy())\n",
    "\n",
    "    tensor_slices = {\"user_id\": [], \"movie_title\": [], \"user_rating\": []}\n",
    "\n",
    "    for user_id, feature_lists in example_lists_by_user.items():\n",
    "        for _ in range(num_list_per_user):\n",
    "\n",
    "            # Drop the user if they don't have enough ratings.\n",
    "            if len(feature_lists[\"movie_title\"]) < num_examples_per_list:\n",
    "                continue\n",
    "\n",
    "            sampled_movie_titles, sampled_ratings = _sample_list(\n",
    "                feature_lists,\n",
    "                num_examples_per_list,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            tensor_slices[\"user_id\"].append(user_id)\n",
    "            tensor_slices[\"movie_title\"].append(sampled_movie_titles)\n",
    "            tensor_slices[\"user_rating\"].append(sampled_ratings)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "# print(f\"train: {train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a36275",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "Current Problems:\n",
    "* int/double features continues or non continous\n",
    "* NOTHING IS TESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7']\n"
     ]
    }
   ],
   "source": [
    "#get all books and users from csv\n",
    "book_df = pd.read_csv('Book_MadeUp.csv')\n",
    "\n",
    "# book_title\n",
    "moive_titles = book_df['movie_titles'].values\n",
    "movie_titles = list(map(str, moive_titles))\n",
    "print(movie_titles)\n",
    "book_description = book_df['book_descriptions'].values\n",
    "book_num_pages = book_df['num_pages'].values\n",
    "book_ratings = book_df['ratings'].values\n",
    "book_genres = book_df['genres'].values# is array/list\n",
    "book_authors = book_df['authors'].values# is array/list\n",
    "\n",
    "### unique values # lloks like  no need for unique values###\n",
    "unique_movie_titles = np.unique(moive_titles)\n",
    "unique_book_description = np.unique(book_description)\n",
    "unique_book_num_pages = np.unique(book_num_pages)\n",
    "unique_book_ratings = np.unique(book_ratings)\n",
    "unique_book_genres = np.unique(book_genres)\n",
    "\n",
    "\n",
    "all_genres = ','.join(book_genres).split(',')\n",
    "book_unique_genres = list(set(all_genres))# should these np.unique  \n",
    "book_unique_authors = list(set(book_authors))# should these np.unique   \n",
    "\n",
    "### unique values ###\n",
    "\n",
    "all_genres = ','.join(book_genres).split(',')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3c89b",
   "metadata": {},
   "source": [
    "##### Book Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x0000027D909C3420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x0000027D909F3BA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# for text\n",
    "\n",
    "# Book title \n",
    "book_title_lookup = tf.keras.layers.StringLookup()\n",
    "book_title_lookup.adapt(movie_titles)# should be unique book titles, the strings\n",
    "\n",
    "#book description\n",
    "book_description_lookup = tf.keras.layers.TextVectorization()\n",
    "book_description_lookup.adapt(book_description) # should be a list of book descriptions (make sure less than 500mb)\n",
    "\n",
    "#num pages # not sure honestly\n",
    "num_pages = tf.keras.layers.Normalization(axis=None)\n",
    "num_pages.adapt(book_num_pages.astype(np.float32)) # list of all num pages\n",
    "\n",
    "#rating # think wrong\n",
    "rating = tf.keras.layers.Normalization(axis=None)\n",
    "rating.adapt(book_ratings.astype(np.float32)) # list of all ratings # type is double\n",
    "\n",
    "# authors\n",
    "book_author_lookup = tf.keras.layers.StringLookup() # WRONG !!!!\n",
    "book_author_lookup.adapt(book_authors) # list of all authors\n",
    "\n",
    "# genres\n",
    "book_genre_lookup = tf.keras.layers.StringLookup()\n",
    "book_genre_lookup.adapt(book_genres) # list of all genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fac678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie model # could test ind\n",
    "\n",
    "class BookModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000 #?\n",
    "\n",
    "        self.book_title_embedding = tf.keras.Sequential([\n",
    "            book_title_lookup,\n",
    "            tf.keras.layers.Embedding(book_title_lookup.vocab_size(), 32)\n",
    "        ])\n",
    "        \n",
    "        # I think needs changing # MAYBE LEAVE OUT FOR NOW\n",
    "        self.book_description_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.TextVectorization(max_tokens=max_tokens),\n",
    "            tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            # We average the embedding of individual words to get one embedding vector\n",
    "            # per title.\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "        \n",
    "        #num pages\n",
    "        self.num_pages_embedding = tf.keras.Sequential([\n",
    "            num_pages\n",
    "        ])\n",
    "        \n",
    "        #rating\n",
    "        self.rating_embedding = tf.keras.Sequential([\n",
    "            rating\n",
    "        ])\n",
    "        \n",
    "        # Preferences as categorical features\n",
    "        self.author_embedding = tf.keras.Sequential([\n",
    "            book_author_lookup,\n",
    "            tf.keras.layers.Embedding(book_author_lookup.vocabulary_size(), 32)\n",
    "        ])\n",
    "\n",
    "        self.book_genre_embedding = tf.keras.Sequential([\n",
    "             book_genre_lookup,\n",
    "            tf.keras.layers.Embedding( book_genre_lookup.vocabulary_size(), 32)\n",
    "        ])\n",
    "     \n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.book_title_embedding(inputs[\"movie_titles\"]),\n",
    "            self.book_description_embeddings(inputs[\"book_descriptions\"]),\n",
    "            self.num_pages_embedding(inputs[\"num_pages\"]),\n",
    "            self.rating_embedding(inputs[\"ratings\"]),\n",
    "            self.book_author_embedding(inputs[\"authors\"]),\n",
    "            self.book_genre_embedding(inputs[\"genres\"])\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343af2b9",
   "metadata": {},
   "source": [
    "##### User Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5']\n",
      "[18. 25. 50. 70. 80.]\n",
      "['classics', 'fiction', 'historical', 'historical fiction', 'italian literature', 'literature', 'mystery', 'childrens', 'classics', 'fantasy', 'fiction', 'juvenile', 'middle grade', 'young adult', 'astronomy', 'history', 'nonfiction', 'philosophy', 'physics', 'science', 'space', 'action', 'adventure', 'espionage', 'fiction', 'mystery', 'suspense', 'thriller', 'childrens', 'classics', 'fantasy', 'fiction', 'juvenile', 'middle grade', 'young adult']\n",
      "[ True  True  True  True  True]\n",
      "['robert ludlum', 'lynne reid banks', 'stephen king', 'robert ludlum', 'lynne reid banks', 'stephen king', 'robert ludlum', 'lynne reid banks', 'stephen king', 'robert ludlum', 'lynne reid banks', 'stephen king', 'robert ludlum', 'lynne reid banks', 'stephen king']\n"
     ]
    }
   ],
   "source": [
    "user_df = pd.read_csv('User_MadeUp.csv')\n",
    "\n",
    "user_ids = user_df['user_id'].values\n",
    "user_ids = list(map(str, user_ids))\n",
    "\n",
    "user_age = user_df['user_age'].values\n",
    "user_genre_pref = user_df['genres'].values\n",
    "user_author_pref = user_df['authors'].values\n",
    "user_sex= user_df['user_sex'].values\n",
    "\n",
    "all_user_genres = ','.join(user_genre_pref).split(',')\n",
    "all_author_pref = ','.join(user_author_pref).split(',')\n",
    "user_age = np.asarray(user_age).astype('float32')\n",
    "\n",
    "print(user_ids)\n",
    "print(user_age)\n",
    "print(all_user_genres)\n",
    "print(user_sex)\n",
    "print(all_author_pref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# User ID \n",
    "user_id_lookup = tf.keras.layers.StringLookup()\n",
    "user_id_lookup.adapt(user_ids)\n",
    "\n",
    "#  User age # TODO: get all user ages as array\n",
    "user_age_lookup = tf.keras.layers.Normalization(axis=None)\n",
    "user_age_lookup.adapt(np.array([user_age])) \n",
    "\n",
    "# Genre Preference :string[]\n",
    "genre_lookup = tf.keras.layers.StringLookup()\n",
    "genre_lookup.adapt(all_user_genres)\n",
    "\n",
    "# Author Preference :string[]\n",
    "author_lookup = tf.keras.layers.StringLookup()\n",
    "author_lookup.adapt(all_author_pref)\n",
    "\n",
    "# TODO: change data types of \n",
    "    # book_length_pref, publication_age_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ada96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOT TESTED\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    # don't these need to be passed in?\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # User embedding based on user_id\n",
    "        self.user_id_embedding = tf.keras.Sequential([\n",
    "            user_id_lookup,\n",
    "            tf.keras.layers.Embedding(user_id_lookup.vocabulary_size(), 32),\n",
    "        ])\n",
    "\n",
    "        # Age as a continuous feature, we'll normalize it\n",
    "        self.age_normalization = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "        # Sex as a boolean feature, could be treated directly as an input or embedded\n",
    "        self.sex_embedding = tf.keras.layers.Embedding(\n",
    "            2, 32)  # Assuming sex is represented as 0 or 1\n",
    "\n",
    "        # Preferences as categorical features\n",
    "        self.author_embedding = tf.keras.Sequential([\n",
    "            author_lookup,\n",
    "            tf.keras.layers.Embedding(author_lookup.vocabulary_size(), 32)\n",
    "        ])\n",
    "\n",
    "        self.genre_embedding = tf.keras.Sequential([\n",
    "            genre_lookup,\n",
    "            tf.keras.layers.Embedding(genre_lookup.vocabulary_size(), 32)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # User feature from user ID\n",
    "        user_id_feature = self.user_id_embedding(inputs[\"user_id\"])\n",
    "\n",
    "        # Process age\n",
    "        age_feature = tf.reshape(\n",
    "            self.age_normalization(inputs[\"age\"]), (-1, 1))\n",
    "\n",
    "        # Process sex\n",
    "        sex_feature = self.sex_embedding(tf.cast(inputs[\"sex\"], tf.int32))\n",
    "\n",
    "        # Process preferences\n",
    "        author_features = self.author_embedding(inputs[\"author\"])# can't be right input = string[]\n",
    "        genre_features = self.genre_embedding(inputs[\"genre\"])# can't be right input = string[]\n",
    "\n",
    "        # Concatenate all features\n",
    "        return tf.concat([\n",
    "            user_id_feature,\n",
    "            age_feature,\n",
    "            sex_feature,\n",
    "            author_features,\n",
    "            genre_features,\n",
    "            format_features\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d2d2d",
   "metadata": {},
   "source": [
    "# Phase 2: Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699be1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRankingModel\u001b[39;00m(\u001b[43mtfrs\u001b[49m\u001b[38;5;241m.\u001b[39mModel):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfrs' is not defined"
     ]
    }
   ],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # User embeddings\n",
    "        self.user_embeddings = UserModel()\n",
    "        # Restaurand embeddings\n",
    "        self.book_embeddings = BookModel()\n",
    "        # Compute predictions\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "            # Learn multiple dense layers.\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "                tf.keras.metrics.RootMeanSquaredError()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, features):  # features Type: Dict{}, Key: UserID, BookID, Value: Tensor\n",
    "        user_embeddings = self.user_model({\n",
    "            'user_id': features['user_id'],\n",
    "            'age': features['user_age'],  # Adjust field name as necessary\n",
    "            'sex': features['user_sex'],  # Adjust field name as necessary\n",
    "            'author': features['author'],  # If applicable\n",
    "            'genre': features['genre'],  # If applicable\n",
    "        })\n",
    "\n",
    "        # Process movie features through the MovieModel\n",
    "        # Adjust field names and structure as necessary to match your MovieModel's expected input\n",
    "        book_embeddings = self.movie_model({\n",
    "            'movie_title': features['movie_title'],\n",
    "            'book_description': features['book_description'],  # If applicable\n",
    "            'num_pages': features['num_pages'],  # If applicable\n",
    "            'rating': features['rating'],  # If applicable\n",
    "            'author': features['author'],  # If applicable\n",
    "            'genre': features['genre'],  # If applicable\n",
    "        })\n",
    "        \n",
    "        user_embeddings_expanded = tf.expand_dims(user_embeddings, 1)\n",
    "    \n",
    "        combined_embeddings = tf.concat([user_embeddings_expanded, movie_embeddings], axis=-1)\n",
    "\n",
    "        return self.score_model(combined_embeddings)\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        labels = features.pop(\"user_rating\")\n",
    "        scores = self(features)\n",
    "        return self.task(\n",
    "            labels=labels,\n",
    "            predictions=tf.squeeze(scores, axis=-1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95f5d8",
   "metadata": {},
   "source": [
    "# Phase 3\n",
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad6d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec={'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'movie_title': TensorSpec(shape=(5,), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(5,), dtype=tf.float64, name=None)}>\n",
      "<CacheDataset element_spec={'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'movie_title': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(None, 5), dtype=tf.float64, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "print (cached_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998963b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python311\\Lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_16956\\3841226348.py\", line 55, in compute_loss\n        scores = self(features)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\__autograph_generated_filetnxq_nvl.py\", line 10, in tf__call\n        user_embeddings = ag__.converted_call(ag__.ld(self).user_model, ({'user_id': ag__.ld(features)['user_id'], 'age': ag__.ld(features)['user_age'], 'sex': ag__.ld(features)['user_sex'], 'author': ag__.ld(features)['author'], 'genre': ag__.ld(features)['genre']},), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'ranking_model_4' (type RankingModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_16956\\3841226348.py\", line 28, in call  *\n            user_embeddings = self.user_model({\n    \n        AttributeError: 'RankingModel' object has no attribute 'user_model'\n    \n    \n    Call arguments received by layer 'ranking_model_4' (type RankingModel):\n      • features={'user_id': 'tf.Tensor(shape=(None,), dtype=string)', 'movie_title': 'tf.Tensor(shape=(None, 5), dtype=string)'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m listwise_model \u001b[38;5;241m=\u001b[39m RankingModel(tfr\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mListMLELoss())\n\u001b[0;32m      2\u001b[0m listwise_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad(\u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mlistwise_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# error here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2eo41z2c.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_recommenders\\models\\base.py:68\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom train step using the `compute_loss` method.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 68\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m   \u001b[38;5;66;03m# Handle regularization losses as well.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m   regularization_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "Cell \u001b[1;32mIn[100], line 55\u001b[0m, in \u001b[0;36mRankingModel.compute_loss\u001b[1;34m(self, features, training)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, features, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     54\u001b[0m     labels \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_rating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask(\n\u001b[0;32m     57\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m     58\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39msqueeze(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     59\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetnxq_nvl.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m user_embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_model\u001b[49m, ({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_age\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_sex\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]},), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m book_embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmovie_model, ({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_title\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_title\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_description\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_description\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_pages\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_pages\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m: ag__\u001b[38;5;241m.\u001b[39mld(features)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]},), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m user_embeddings_expanded \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mld(user_embeddings), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python311\\Lib\\site-packages\\tensorflow_recommenders\\models\\base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_16956\\3841226348.py\", line 55, in compute_loss\n        scores = self(features)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\__autograph_generated_filetnxq_nvl.py\", line 10, in tf__call\n        user_embeddings = ag__.converted_call(ag__.ld(self).user_model, ({'user_id': ag__.ld(features)['user_id'], 'age': ag__.ld(features)['user_age'], 'sex': ag__.ld(features)['user_sex'], 'author': ag__.ld(features)['author'], 'genre': ag__.ld(features)['genre']},), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'ranking_model_4' (type RankingModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_16956\\3841226348.py\", line 28, in call  *\n            user_embeddings = self.user_model({\n    \n        AttributeError: 'RankingModel' object has no attribute 'user_model'\n    \n    \n    Call arguments received by layer 'ranking_model_4' (type RankingModel):\n      • features={'user_id': 'tf.Tensor(shape=(None,), dtype=string)', 'movie_title': 'tf.Tensor(shape=(None, 5), dtype=string)'}\n"
     ]
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "listwise_model.fit(cached_train, epochs=100, verbose=True)  # error here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
