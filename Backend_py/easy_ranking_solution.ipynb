{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x00000288FFA90C90>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import keras\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from mysql.connector import connect, Error\n",
    "\n",
    "# Connect to database\n",
    "try:\n",
    "    with connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"mysql\",\n",
    "        database=\"Sprint1BasicEComDb\"\n",
    "    ) as connection:\n",
    "        print(connection)\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ratings(): #TODO: like get All Books, add attributes to the query\n",
    "    query = \"SELECT userId,bookId,rating FROM UserBookRatings\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    user_ratings = mydb.fetchall()\n",
    "    return user_ratings\n",
    "\n",
    "\n",
    "def get_all_books_metadata():\n",
    "    query =\"\"\" \n",
    "    \"\"\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    all_books = mydb.fetchall()\n",
    "    return all_books\n",
    "\n",
    "\n",
    "def get_all_users_metadata():\n",
    "    query = \"SELECT id,birthDate,sex FROM Users\"\n",
    "    # incase connection is lost, reconnect\n",
    "    connection.reconnect(attempts=3, delay=5)\n",
    "    mydb = connection.cursor()\n",
    "    mydb.execute(query)\n",
    "    all_users = mydb.fetchall()\n",
    "    return all_users\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename, header):\n",
    "    user_ratings_ids = pd.DataFrame(data).set_axis(header, axis=1)\n",
    "    user_ratings_ids.to_csv(filename, index=False, )\n",
    "    \n",
    "def newUserIDs(maxID,quantity):\n",
    "    newID = []\n",
    "    for i in range(quantity):\n",
    "        newID.append(maxID+i+1)\n",
    "    return newID\n",
    "    \n",
    "save_to_csv(get_user_ratings(), 'user_ratings.csv', ['user_id', 'movie_title', 'rating'])\n",
    "#save_to_csv(get_all_books(), 'all_books.csv', ['book_id', 'book_title', 'description', 'num_pages', 'rating', 'num_of_voters','genres','formats','authors'])\n",
    "#save_to_csv(get_all_users(), 'all_users.csv', ['user_id', 'birth_date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_12596\\1318318014.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new_users_known_books['user_id'] = df_new_users_known_books['user_id'].astype(str)\n",
      "C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_12596\\1318318014.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new_users_known_books['movie_title'] = df_new_users_known_books['movie_title'].astype(str)\n",
      "C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_12596\\1318318014.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_old_users_new_books['user_id'] = df_old_users_new_books['user_id'].astype(str)\n",
      "C:\\Users\\DZera\\AppData\\Local\\Temp\\ipykernel_12596\\1318318014.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_old_users_new_books['movie_title'] = df_old_users_new_books['movie_title'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "df = get_user_ratings()\n",
    "\n",
    "user_ratings_ids = pd.DataFrame(df, columns=[\"user_id\", \"movie_title\", \"user_rating\"])\n",
    "\n",
    "users_to_remove = user_ratings_ids['user_id'].drop_duplicates().sample(10)\n",
    "books_to_remove = user_ratings_ids['movie_title'].drop_duplicates().sample(50)\n",
    "df_new_users_known_books = user_ratings_ids[~user_ratings_ids['user_id'].isin(users_to_remove) & ~user_ratings_ids['movie_title'].isin(books_to_remove)]\n",
    "df_old_users_new_books = user_ratings_ids[user_ratings_ids['user_id'].isin(users_to_remove) & ~user_ratings_ids['movie_title'].isin(books_to_remove)]\n",
    "\n",
    "# convert to string\n",
    "user_ratings_ids['user_id'] = user_ratings_ids['user_id'].astype(str)\n",
    "user_ratings_ids['movie_title'] = user_ratings_ids['movie_title'].astype(str)\n",
    "\n",
    "df_new_users_known_books['user_id'] = df_new_users_known_books['user_id'].astype(str)\n",
    "df_new_users_known_books['movie_title'] = df_new_users_known_books['movie_title'].astype(str)\n",
    "\n",
    "df_old_users_new_books['user_id'] = df_old_users_new_books['user_id'].astype(str)\n",
    "df_old_users_new_books['movie_title'] = df_old_users_new_books['movie_title'].astype(str)\n",
    "\n",
    "########\n",
    "\n",
    "rating_rank = user_ratings_ids[['user_id', 'movie_title', 'user_rating']].copy()\n",
    "book_rank = user_ratings_ids[['movie_title']].copy()\n",
    "\n",
    "rating_rank10 = df_new_users_known_books[['user_id', 'movie_title', 'user_rating']].copy()\n",
    "book_rank10 = df_new_users_known_books[['movie_title']].copy()\n",
    "\n",
    "rating_rank01 = df_old_users_new_books[['user_id', 'movie_title', 'user_rating']].copy()\n",
    "book_rank01 = df_old_users_new_books[['movie_title']].copy()\n",
    "\n",
    "user_ratings_ids=[]\n",
    "df_new_users_known_books=[]\n",
    "df_old_users_new_books=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset from the dataframe\n",
    "# Will cause error if ran again\n",
    "\n",
    "\n",
    "books = tf.data.Dataset.from_tensor_slices(dict(book_rank)) # book_rank only \n",
    "books = books.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "books10 = tf.data.Dataset.from_tensor_slices(dict(book_rank10)) # book_rank only # problem\n",
    "books10 = books10.map(lambda x: x[\"movie_title\"]) # \n",
    "\n",
    "books01 = tf.data.Dataset.from_tensor_slices(dict(book_rank01)) # book_rank only \n",
    "books01 = books01.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rating_rank = tf.data.Dataset.from_tensor_slices(dict(rating_rank))\n",
    "rating_rank = rating_rank.map(lambda x: {\n",
    "    'user_id': x['user_id'],\n",
    "    'movie_title': x['movie_title'],\n",
    "    'user_rating': x['user_rating'],\n",
    "})\n",
    "\n",
    "rating_rank10 = tf.data.Dataset.from_tensor_slices(dict(rating_rank10))\n",
    "rating_rank10 = rating_rank10.map(lambda x: {\n",
    "    'user_id': x['user_id'],\n",
    "    'movie_title': x['movie_title'],\n",
    "    'user_rating': x['user_rating'],\n",
    "})\n",
    "\n",
    "rating_rank01 = tf.data.Dataset.from_tensor_slices(dict(rating_rank01))\n",
    "rating_rank01 = rating_rank01.map(lambda x: {\n",
    "    'user_id': x['user_id'],\n",
    "    'movie_title': x['movie_title'],\n",
    "    'user_rating': x['user_rating'],\n",
    "})\n",
    "\n",
    "## new \n",
    "\n",
    "unique_books = np.unique(np.concatenate(list(books.batch(1000)))) # is it correct IDK # should be strings\n",
    "unique_books10 = np.unique(np.concatenate(list(books10.batch(1000)))) # is it correct IDK # should be strings\n",
    "unique_books01 = np.unique(np.concatenate(list(books01.batch(1000)))) # is it correct IDK # should be strings\n",
    "\n",
    "unique_user_ids = np.unique(np.concatenate(list(rating_rank.batch(1_000).map(lambda x: x['user_id'])))) ## could be book id\n",
    "unique_user_ids10 = np.unique(np.concatenate(list(rating_rank10.batch(1_000).map(lambda x: x['user_id']))))\n",
    "unique_user_ids01 = np.unique(np.concatenate(list(rating_rank01.batch(1_000).map(lambda x: x['user_id']))))\n",
    "\n",
    "# decode from bytes to string\n",
    "unique_books = [book.decode('utf-8') for book in unique_books]\n",
    "unique_user_ids = [user_id.decode('utf-8') for user_id in unique_user_ids]\n",
    "\n",
    "unique_books10 = [book.decode('utf-8') for book in unique_books10]\n",
    "unique_user_ids10 = [user_id.decode('utf-8') for user_id in unique_user_ids10]\n",
    "\n",
    "unique_books01 = [book.decode('utf-8') for book in unique_books01]\n",
    "unique_user_ids01 = [user_id.decode('utf-8') for user_id in unique_user_ids01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "# do we have 100_000 ratings? # user_ratings needs to be a new type\n",
    "shuffled = rating_rank.shuffle(\n",
    "    100, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "shuffled10 = rating_rank10.shuffle(\n",
    "    5, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "shuffled01 = rating_rank01.shuffle(\n",
    "    5, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "\n",
    "\n",
    "train = shuffled.take(70)\n",
    "test = shuffled.skip(70).take(30)\n",
    "\n",
    "train10 = shuffled10.take(100)\n",
    "test10 = shuffled10.skip(3).take(2)\n",
    "print(len(train10))\n",
    "\n",
    "train01 = shuffled01.take(10)\n",
    "test01 = shuffled01.skip(3).take(2)\n",
    "\n",
    "\n",
    "\n",
    "# We sample 50 lists for each user for the training data. For each list we\n",
    "# sample 5 movies from the movies the user rated.\n",
    "train = tfrs.examples.movielens.sample_listwise(\n",
    "    train,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "test = tfrs.examples.movielens.sample_listwise( ## making test empty\n",
    "    test,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train10 = tfrs.examples.movielens.sample_listwise(\n",
    "    train10,\n",
    "    num_list_per_user=100,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test10 = tfrs.examples.movielens.sample_listwise( ## making test empty\n",
    "    test10,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train01 = tfrs.examples.movielens.sample_listwise(\n",
    "    train01,\n",
    "    num_list_per_user=100,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test01 = tfrs.examples.movielens.sample_listwise( ## making test empty\n",
    "    test01,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "print(len(train10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_books),\n",
    "      tf.keras.layers.Embedding(len(unique_books) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "    # tensor.\n",
    "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
    "\n",
    "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the user\n",
    "    # embeddings to match the shape of movie embeddings.\n",
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, movie_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"user_rating\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "cached_train = train.shuffle(100).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "\n",
    "cached_train10 = train10.shuffle(50).batch(8192).cache()\n",
    "cached_test10 = test10.batch(4096).cache()\n",
    "\n",
    "print(len(cached_train10))\n",
    "\n",
    "cached_train01 = train01.shuffle(100).batch(8192).cache()\n",
    "cached_test01 = test01.batch(4096).cache()\n",
    "\n",
    "print(len(cached_train01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 5s 5s/step - ndcg_metric: 0.5494 - root_mean_squared_error: 1.5308 - loss: 4.7980 - regularization_loss: 0.0000e+00 - total_loss: 4.7980\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 15ms/step - ndcg_metric: 1.0000 - root_mean_squared_error: 1.5405 - loss: 4.6759 - regularization_loss: 0.0000e+00 - total_loss: 4.6759\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 15ms/step - ndcg_metric: 1.0000 - root_mean_squared_error: 1.5145 - loss: 4.4383 - regularization_loss: 0.0000e+00 - total_loss: 4.4383\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 14ms/step - ndcg_metric: 1.0000 - root_mean_squared_error: 1.3084 - loss: 3.9656 - regularization_loss: 0.0000e+00 - total_loss: 3.9656\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 14ms/step - ndcg_metric: 1.0000 - root_mean_squared_error: 1.1350 - loss: 2.9811 - regularization_loss: 0.0000e+00 - total_loss: 2.9811\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 21ms/step - ndcg_metric: 0.7552 - root_mean_squared_error: 2.3002 - loss: 5.7776 - regularization_loss: 0.0000e+00 - total_loss: 5.7776\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 18ms/step - ndcg_metric: 0.8718 - root_mean_squared_error: 2.1949 - loss: 4.1392 - regularization_loss: 0.0000e+00 - total_loss: 4.1392\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 19ms/step - ndcg_metric: 0.9428 - root_mean_squared_error: 2.2125 - loss: 3.3689 - regularization_loss: 0.0000e+00 - total_loss: 3.3689\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 18ms/step - ndcg_metric: 0.9453 - root_mean_squared_error: 2.1929 - loss: 3.1562 - regularization_loss: 0.0000e+00 - total_loss: 3.1562\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 17ms/step - ndcg_metric: 0.9453 - root_mean_squared_error: 2.1826 - loss: 2.9988 - regularization_loss: 0.0000e+00 - total_loss: 2.9988\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input data to be non-empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m listwise_model\u001b[38;5;241m.\u001b[39mfit(cached_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[0;32m      4\u001b[0m listwise_model\u001b[38;5;241m.\u001b[39mfit(cached_train10, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[1;32m----> 5\u001b[0m \u001b[43mlistwise_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_train01\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1319\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_dataset_and_inferred_steps(\n\u001b[0;32m   1315\u001b[0m     strategy, x, steps_per_epoch, class_weight, distribute\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input data to be non-empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input data to be non-empty."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "listwise_model.fit(cached_train, epochs=5, verbose=True)  \n",
    "listwise_model.fit(cached_train10, epochs=5, verbose=True)  \n",
    "listwise_model.fit(cached_train01, epochs=5, verbose=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listwise_model_result = listwise_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"NDCG of the ListMLE model: {:.4f}\".format(listwise_model_result[\"ndcg_metric\"]))\n",
    "listwise_model.save(\"easy_listwise_model_saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = listwise_model({\n",
    "    \"user_id\": tf.constant([\"42\"]),\n",
    "    \"movie_title\": tf.constant([[\"1\", \"2\", \"3\", \"4\", \"5\"]])\n",
    "})\n",
    "\n",
    "loaded_model = keras.models.load_model('easy_listwise_model_saved')\n",
    "\n",
    "\n",
    "def make_predictions(model, user_id, books_lists): # model, user_id:str[], books_lists:str[[]], has to be equal lengths\n",
    "    predictions = []\n",
    "    for i in range(len(books_lists)):\n",
    "        predictions.append( model({\n",
    "            \"user_id\": tf.constant([user_id[i]]),\n",
    "            \"movie_title\": tf.constant([books_lists[i]])\n",
    "        }))\n",
    "    return predictions\n",
    "\n",
    "predictions = make_predictions(listwise_model, [\"1000\",\"2\"], [[\"10000\", \"2\", \"3\", \"4\", \"5\"],[\"1\", \"2\", \"3\", \"4\", \"5\"]])\n",
    "\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
